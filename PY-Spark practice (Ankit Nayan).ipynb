{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4655c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45f65a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed3fb63",
   "metadata": {},
   "source": [
    "https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816c46cc",
   "metadata": {},
   "source": [
    "# 1. PySpark Read CSV File into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ad902fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-3acdd7f77853>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-3acdd7f77853>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    val spark = SparkSession.builder().master(\"local[1]\")\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "val spark = SparkSession.builder().master(\"local[1]\")\n",
    "          .appName(\"SparkByExamples.com\")\n",
    "          .getOrCreate()\n",
    "df = spark.read.csv(\"/tmp/resources/zipcodes.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c244f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\n",
    "                  .load(\"enter your path/tmp/resources/zipcodes.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782953d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"org.apache.spark.sql.csv\")\n",
    "                  .load(\"/tmp/resources/zipcodes.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc685fc4",
   "metadata": {},
   "source": [
    "#you can also specify the Data sources by their fully qualified name, \n",
    "#but for built-in sources, you can simply use their short names (csv,json, parquet, jdbc, text e.t.c). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b436055",
   "metadata": {},
   "source": [
    "# 2. Options While Reading CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab696da9",
   "metadata": {},
   "source": [
    "PySpark CSV dataset provides multiple options to work with CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319be8d",
   "metadata": {},
   "source": [
    "1. option(self, key, value)\n",
    "2. options(self, **options)\n",
    "\n",
    "Note - inferschema (2.2) me example dekho smjhne ke liye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9019baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Multiple CSV Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a419ec0",
   "metadata": {},
   "source": [
    "Using the read.csv() method you can also read multiple csv files, \n",
    "just pass all file names by separating comma as a path, for example : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebc39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"path1,path2,path3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341efb39",
   "metadata": {},
   "source": [
    "2.1 delimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349e5cd",
   "metadata": {},
   "source": [
    "delimiter option is used to specify the column delimiter of the CSV file. By default, it is comma (,) character, but can be set to any character like pipe(|), tab (\\t), space using this option.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1431ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.options(delimiter=',') \\\n",
    "  .csv(\"C:/apps/sparkbyexamples/src/pyspark-examples/resources/zipcodes.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf391ea",
   "metadata": {},
   "source": [
    "2.2 inferSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e1489e",
   "metadata": {},
   "source": [
    "The default value set to this option is False when setting to true it automatically infers (parinam nikalna) column types based on the data. \n",
    "\n",
    "Note:-  it requires reading the data one more time to infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.read.options(inferSchema='True',delimiter=',') \\\n",
    "  .csv(\"src/main/resources/zipcodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02035bd2",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8961cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.read.option(\"inferSchema\",True) \\\n",
    "                .option(\"delimiter\",\",\") \\\n",
    "  .csv(\"src/main/resources/zipcodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fde577",
   "metadata": {},
   "source": [
    "2.3 header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e8566",
   "metadata": {},
   "source": [
    "This option is used to read the first line of the CSV file as column names. By default the value of this option is False , and all column types are assumed to be a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d64857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "  .csv(\"/tmp/resources/zipcodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e420425",
   "metadata": {},
   "source": [
    "2.4 quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5130b8",
   "metadata": {},
   "source": [
    "When you have a column with a delimiter that used to split the columns, use quotes option to specify the quote character, by default it is ” and delimiters inside quotes are ignored. but using this option you can set any character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6457a73",
   "metadata": {},
   "source": [
    "2.5 nullValues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03248539",
   "metadata": {},
   "source": [
    "Using nullValues option you can specify the string in a CSV to consider as null. For example, if you want to consider a date column with a value \"1900-01-01\" set null on DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea07817",
   "metadata": {},
   "source": [
    "# 3. Reading CSV files with a user-specified custom schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30df287",
   "metadata": {},
   "outputs": [],
   "source": [
    "agar tum inferschema use nhi karna chahte ho. \n",
    "then use user-defined custom coulmn names and type using schema options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfc3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\     #yha par maine apna schema bna liya hai. now ab isko mai use kar sakta hun df ko read karte samay.\n",
    "      .add(\"RecordNumber\",IntegerType(),True) \\\n",
    "      .add(\"Zipcode\",IntegerType(),True) \\\n",
    "      .add(\"ZipCodeType\",StringType(),True) \\\n",
    "      .add(\"City\",StringType(),True) \\\n",
    "      .add(\"State\",StringType(),True) \\\n",
    "      .add(\"LocationType\",StringType(),True) \\\n",
    "      .add(\"Lat\",DoubleType(),True) \\\n",
    "      .add(\"Long\",DoubleType(),True) \\\n",
    "      .add(\"Xaxis\",IntegerType(),True) \\\n",
    "      .add(\"Yaxis\",DoubleType(),True) \\\n",
    "      .add(\"Zaxis\",DoubleType(),True) \\\n",
    "      .add(\"WorldRegion\",StringType(),True) \\\n",
    "      .add(\"Country\",StringType(),True) \\\n",
    "      .add(\"LocationText\",StringType(),True) \\\n",
    "      .add(\"Location\",StringType(),True) \\\n",
    "      .add(\"Decommisioned\",BooleanType(),True) \\\n",
    "      .add(\"TaxReturnsFiled\",StringType(),True) \\\n",
    "      .add(\"EstimatedPopulation\",IntegerType(),True) \\\n",
    "      .add(\"TotalWages\",IntegerType(),True) \\\n",
    "      .add(\"Notes\",StringType(),True)\n",
    "      \n",
    "df_with_schema = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/tmp/resources/zipcodes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c0783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c54ce29e",
   "metadata": {},
   "source": [
    "# 4. Write PySpark DataFrame to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a91243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"header\",True) \\\n",
    "  .csv(\"/tmp/spark_output/zipcodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ab211",
   "metadata": {},
   "source": [
    "4.1 Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44eed07",
   "metadata": {},
   "source": [
    "While writing a CSV file you can use several options. \n",
    "\n",
    "for example:-\n",
    "\n",
    "header :- to output the DataFrame column names as header record.\n",
    "\n",
    "delimiter :- to specify the delimiter on the CSV output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa52726",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.options(header='True', delimiter=',') \\\n",
    "   .csv(\"/tmp/spark_output/zipcodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c9b4d",
   "metadata": {},
   "source": [
    "4.2 Saving modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279dc6b",
   "metadata": {},
   "source": [
    "PySpark DataFrameWriter also has a method 'mode()' to specify saving mode.\n",
    "\n",
    "overwrite – mode is used to overwrite the existing file.\n",
    "\n",
    "append – To add the data to the existing file.\n",
    "\n",
    "ignore – Ignores write operation when the file already exists.\n",
    "\n",
    "error – This is a default option when the file already exists, it returns an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edaa419",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.format(\"csv\").mode('overwrite').save(\"/tmp/spark_output/zipcodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef2d2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d637ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61b78055",
   "metadata": {},
   "source": [
    "# PySpark – Create DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f8d57",
   "metadata": {},
   "source": [
    "In order to create a DataFrame from a list \n",
    "we need the data hence, \n",
    "first, let’s create the \"data\" and the \"columns\" that are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1355c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d96561",
   "metadata": {},
   "source": [
    "1. Create DataFrame from RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60157f44",
   "metadata": {},
   "source": [
    "Create PySpark DataFrame is from an existing RDD. \n",
    "\n",
    "First, let’s create a Spark RDD from a collection List by calling parallelize() function from SparkContext . \n",
    "\n",
    "We would need this rdd object for all our examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19060cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06357f6",
   "metadata": {},
   "source": [
    "1.1 Using \"toDF()\" function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4d70f",
   "metadata": {},
   "source": [
    "-PySpark RDD’s toDF() method is used to create a DataFrame from existing RDD. \n",
    "\n",
    "-Since RDD doesn’t have columns, \n",
    "the DataFrame is created with default column names :-\n",
    "\n",
    "“_1” \n",
    "\n",
    "and “_2” \n",
    "\n",
    "as we have two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30138f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD1 = rdd.toDF()\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683554c4",
   "metadata": {},
   "source": [
    "If you wanted to provide column names to the DataFrame use \n",
    "\n",
    "---toDF() method with column names as arguments as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "808db540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "dfFromRDD1 = rdd.toDF(columns)\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb07b1f",
   "metadata": {},
   "source": [
    "1.2 Using \"createDataFrame()\" from SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63e5a93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFromRDD2 = spark.createDataFrame(rdd).toDF(*columns)\n",
    "dfFromRDD2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea652a90",
   "metadata": {},
   "source": [
    "2.3 Create DataFrame with schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e429497f",
   "metadata": {},
   "source": [
    "StructType schema==To specify the column names along with their data types, \n",
    "you should create the StructType schema first and then \n",
    "assign this while creating a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b344b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cef3ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|id   |gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|James    |          |Smith   |36636|M     |3000  |\n",
      "|Michael  |Rose      |        |40288|M     |4000  |\n",
      "|Robert   |          |Williams|42114|M     |4000  |\n",
      "|Maria    |Anne      |Jones   |39192|F     |4000  |\n",
      "|Jen      |Mary      |Brown   |     |F     |-1    |\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "data2 = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "schema = StructType([ \\   #To specify the column names with their data types\n",
    "    StructField(\"firstname\",StringType(),True), \\\n",
    "    StructField(\"middlename\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"id\", StringType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "    StructField(\"salary\", IntegerType(), True) \\\n",
    "  ])\n",
    " \n",
    "df = spark.createDataFrame(data=data2,schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5860347e",
   "metadata": {},
   "source": [
    "3. Create DataFrame from Data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7db1d",
   "metadata": {},
   "source": [
    "Create DataFrame from data source files like CSV, Text, JSON, XML e.t.c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bd1dd0",
   "metadata": {},
   "source": [
    "3.1 Creating DataFrame from CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae86c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(\"/src/resources/file.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df0cb9",
   "metadata": {},
   "source": [
    "3.2. Creating from text (TXT) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.text(\"/src/resources/file.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ec034e",
   "metadata": {},
   "source": [
    "3.3. Creating from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac25849",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.json(\"/src/resources/file.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaf0639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a4d221a",
   "metadata": {},
   "source": [
    "# HANDS ON PARQUET FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8160db",
   "metadata": {},
   "source": [
    "create a Pyspark DataFrame from a list of data using \n",
    "\n",
    "spark.createDataFrame() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4352f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/ankitnayan/Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df22828a",
   "metadata": {},
   "source": [
    "Pyspark Write DataFrame to Parquet file format(ye automatically file bna dega parquet file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29b82e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"/Users/ankitnayan/Downloads/sample1.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feadb97e",
   "metadata": {},
   "source": [
    "Pyspark Read Parquet file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34b916f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parDF=spark.read.parquet(\"/Users/ankitnayan/Downloads/sample1.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d671e614",
   "metadata": {},
   "source": [
    "Append or Overwrite an existing Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27181515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode('append').parquet(\"/Users/ankitnayan/Downloads/sample1.parquet\")\n",
    "df.write.mode('overwrite').parquet(\"/Users/ankitnayan/Downloads/sample1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "899c89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "parDF.createOrReplaceTempView(\"ParquetTable\")\n",
    "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c8e45e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON USING parquet OPTIONS (path \\\"/tmp/output/people.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e50c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"/Users/ankitnayan/Downloads/sample1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb78e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a29b9724",
   "metadata": {},
   "source": [
    "# PySpark Select Columns From DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1545b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdd07dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a Dataframe.\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe30ac5",
   "metadata": {},
   "source": [
    "1. Select Single & Multiple Columns From PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfbf90",
   "metadata": {},
   "source": [
    "select the single or multiple columns of the DataFrame by passing the column names you wanted to select to the select() function\n",
    "\n",
    "show() function is used to show the Dataframe contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6815430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"firstname\",\"lastname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1aa54fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.firstname,df.lastname).show()\n",
    "df.select(df[\"firstname\"],df[\"lastname\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f6c17af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    James|   Smith|\n",
      "|  Michael|    Rose|\n",
      "|   Robert|Williams|\n",
      "|    Maria|   Jones|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#By using col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"firstname\"),col(\"lastname\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba4c52",
   "metadata": {},
   "source": [
    "2. Select All Columns From List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a9e3534",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select All columns from List\n",
    "df.select(*columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fc21397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|    James|   Smith|    USA|   CA|\n",
      "|  Michael|    Rose|    USA|   NY|\n",
      "|   Robert|Williams|    USA|   CA|\n",
      "|    Maria|   Jones|    USA|   FL|\n",
      "+---------+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select All columns\n",
    "df.select([col for col in df.columns]).show()\n",
    "df.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b92704",
   "metadata": {},
   "source": [
    "3. Select Columns by Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf9af784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+\n",
      "|firstname|lastname|country|\n",
      "+---------+--------+-------+\n",
      "|    James|   Smith|    USA|\n",
      "|  Michael|    Rose|    USA|\n",
      "|   Robert|Williams|    USA|\n",
      "+---------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+-----+\n",
      "|country|state|\n",
      "+-------+-----+\n",
      "|    USA|   CA|\n",
      "|    USA|   NY|\n",
      "|    USA|   CA|\n",
      "+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selects first 3 columns and top 3 rows\n",
    "df.select(df.columns[:3]).show(3)\n",
    "\n",
    "#Selects columns 2 to 4  and top 3 rows\n",
    "df.select(df.columns[2:4]).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee5c172",
   "metadata": {},
   "source": [
    "4. Select Nested Struct Columns from PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd7ae2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+-----+------+\n",
      "|name                  |state|gender|\n",
      "+----------------------+-----+------+\n",
      "|{James, null, Smith}  |OH   |M     |\n",
      "|{Anna, Rose, }        |NY   |F     |\n",
      "|{Julia, , Williams}   |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |NY   |M     |\n",
      "|{Mike, Mary, Williams}|OH   |M     |\n",
      "+----------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
    "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
    "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
    "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
    "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
    "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
    "        ]\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType        \n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "         StructField('firstname', StringType(), True),\n",
    "         StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "         ])),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('gender', StringType(), True)\n",
    "     ])\n",
    "df2 = spark.createDataFrame(data = data, schema = schema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False) # shows all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e9509be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|name                  |\n",
      "+----------------------+\n",
      "|{James, null, Smith}  |\n",
      "|{Anna, Rose, }        |\n",
      "|{Julia, , Williams}   |\n",
      "|{Maria, Anne, Jones}  |\n",
      "|{Jen, Mary, Brown}    |\n",
      "|{Mike, Mary, Williams}|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"name\").show(truncate=False) #let’s select struct column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "430a6a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|James    |Smith   |\n",
      "|Anna     |        |\n",
      "|Julia    |Williams|\n",
      "|Maria    |Jones   |\n",
      "|Jen      |Brown   |\n",
      "|Mike     |Williams|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to select the specific column from a nested struct, \n",
    "#you need to explicitly qualify the nested struct column name.\n",
    "df2.select(\"name.firstname\",\"name.lastname\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a506c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+\n",
      "|firstname|middlename|lastname|\n",
      "+---------+----------+--------+\n",
      "|James    |null      |Smith   |\n",
      "|Anna     |Rose      |        |\n",
      "|Julia    |          |Williams|\n",
      "|Maria    |Anne      |Jones   |\n",
      "|Jen      |Mary      |Brown   |\n",
      "|Mike     |Mary      |Williams|\n",
      "+---------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"name.*\").show(truncate=False) #to get all columns from struct column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d8fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab86cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3134a61e",
   "metadata": {},
   "source": [
    "# PySpark withColumn() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e87e2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b9493a",
   "metadata": {},
   "source": [
    "1. Change DataType using PySpark withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76b5730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60589d",
   "metadata": {},
   "source": [
    "2. Update The Value of an Existing Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3b47dc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|300000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|400000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|400000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|400000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|  -100|\n",
      "+---------+----------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"salary\",col(\"salary\")*100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594ea59",
   "metadata": {},
   "source": [
    "3. Create a Column from an Existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04253093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b29fa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|CopiedColumn|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|       -3000|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|       -4000|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|       -4000|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|       -4000|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|           1|\n",
      "+---------+----------+--------+----------+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"CopiedColumn\",col(\"salary\")* -1).show()\n",
    "#creates a new column “CopiedColumn” by multiplying “salary” column with value -1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291f418",
   "metadata": {},
   "source": [
    "4. Add a New Column using withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bf383b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b74a9e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|Country|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|    USA|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|    USA|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|    USA|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA|\n",
      "+---------+----------+--------+----------+------+------+-------+\n",
      "\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "|firstname|middlename|lastname|       dob|gender|salary|Country|anotherColumn|\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "|    James|          |   Smith|1991-04-01|     M|  3000|    USA| anotherValue|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|  4000|    USA| anotherValue|\n",
      "|   Robert|          |Williams|1978-09-05|     M|  4000|    USA| anotherValue|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|    USA| anotherValue|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|    USA| anotherValue|\n",
      "+---------+----------+--------+----------+------+------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PySpark \"lit()\"\" function is used to add a constant value to a DataFrame column. \n",
    "df.withColumn(\"Country\", lit(\"USA\")).show()\n",
    "df.withColumn(\"Country\", lit(\"USA\")) \\\n",
    "  .withColumn(\"anotherColumn\",lit(\"anotherValue\")) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c9dfe",
   "metadata": {},
   "source": [
    "5. Rename Column Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51691f7",
   "metadata": {},
   "source": [
    "To rename an existing column use \"withColumnRenamed()\"\" function on DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0860e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+---+------+\n",
      "|firstname|middlename|lastname|dob       |sex|salary|\n",
      "+---------+----------+--------+----------+---+------+\n",
      "|James    |          |Smith   |1991-04-01|M  |3000  |\n",
      "|Michael  |Rose      |        |2000-05-19|M  |4000  |\n",
      "|Robert   |          |Williams|1978-09-05|M  |4000  |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F  |4000  |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F  |-1    |\n",
      "+---------+----------+--------+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"gender\",\"sex\") \\\n",
    "  .show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166bdeb",
   "metadata": {},
   "source": [
    "6. Drop Column From PySpark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c99f8",
   "metadata": {},
   "source": [
    "Use “drop” function to drop a specific column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9e705a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+\n",
      "|firstname|middlename|lastname|       dob|gender|\n",
      "+---------+----------+--------+----------+------+\n",
      "|    James|          |   Smith|1991-04-01|     M|\n",
      "|  Michael|      Rose|        |2000-05-19|     M|\n",
      "|   Robert|          |Williams|1978-09-05|     M|\n",
      "|    Maria|      Anne|   Jones|1967-12-01|     F|\n",
      "|      Jen|      Mary|   Brown|1980-02-17|     F|\n",
      "+---------+----------+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"salary\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a23af2",
   "metadata": {},
   "source": [
    "# PySpark \"withColumnRenamed\" to Rename Column on DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76974554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d83c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
    "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
    "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
    "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
    "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3490e5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98726e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "schema = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "         StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('gender', IntegerType(), True)\n",
    "         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f18a3527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df = spark.createDataFrame(data = dataDF, schema = schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec54a99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5fc3423",
   "metadata": {},
   "source": [
    "1. PySpark withColumnRenamed – To rename DataFrame column name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaee7ddd",
   "metadata": {},
   "source": [
    " PySpark withColumnRenamed() Syntax:\n",
    "\n",
    "\n",
    "\"\"withColumnRenamed(existingName, newNam)\"\"\n",
    "\n",
    " existingName – The existing column name you want to change\n",
    " \n",
    " newName – New name of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0905c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed(\"dob\",\"DateOfBirth\").printSchema()# Returns a new DataFrame with a column renamed.\n",
    "# above statement changes column “dob” to “DateOfBirth”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8a350",
   "metadata": {},
   "source": [
    "2. PySpark withColumnRenamed – To rename multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "867f7f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumnRenamed(\"dob\",\"DateOfBirth\") \\\n",
    "        .withColumnRenamed(\"salary\",\"salary_amount\")\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3381d6f",
   "metadata": {},
   "source": [
    "3. Using PySpark StructType – To rename a nested column in Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5b47a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing a column name on nested data is not straight forward\n",
    "# 1.by creating a new schema\n",
    "# 2.use it using cast function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d19510da",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema2 = StructType([\n",
    "    StructField(\"fname\",StringType()),\n",
    "    StructField(\"middlename\",StringType()),\n",
    "    StructField(\"lname\",StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(col(\"name\").cast(schema2), \\ #name me hi change mara hai iske liye, cast me upar wala schema2 daal diya hai\n",
    "     col(\"dob\"), col(\"gender\"),col(\"salary\")) \\ # dob,gender,salary me koi change nhi hai\n",
    "   .printSchema()  \n",
    "# This statement renames firstname to fname and lastname \n",
    "# to lname within name structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f24f0",
   "metadata": {},
   "source": [
    "4. Using Select – To rename nested elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1502d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(col(\"name.firstname\").alias(\"fname\"), \\\n",
    "  col(\"name.middlename\").alias(\"mname\"), \\\n",
    "  col(\"name.lastname\").alias(\"lname\"), \\\n",
    "  col(\"dob\"),col(\"gender\"),col(\"salary\")) \\\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24fc0d",
   "metadata": {},
   "source": [
    "5. Using PySpark DataFrame withColumn – To rename nested columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "790e9007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- gender: integer (nullable = true)\n",
      " |-- fname: string (nullable = true)\n",
      " |-- mname: string (nullable = true)\n",
      " |-- lname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df4 = df.withColumn(\"fname\",col(\"name.firstname\")) \\\n",
    "      .withColumn(\"mname\",col(\"name.middlename\")) \\\n",
    "      .withColumn(\"lname\",col(\"name.lastname\")) \\\n",
    "      .drop(\"name\")\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1438aecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0936bc98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4e97118",
   "metadata": {},
   "source": [
    "# PySpark Where Filter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad85e6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField \n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    " ]\n",
    "        \n",
    "schema = StructType([\n",
    "     StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "     StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    " ])\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbdf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ad6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0488cbd8",
   "metadata": {},
   "source": [
    "2. DataFrame filter() with Column Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2956b16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using equals condition\n",
    "df.filter(df.state == \"OH\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "434fbb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|name                |languages         |state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
      "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|name                |languages         |state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |\n",
      "|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# not equals condition\n",
    "df.filter(df.state != \"OH\").show(truncate=False) \n",
    "df.filter(~(df.state == \"OH\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f76762",
   "metadata": {},
   "source": [
    "5. Filter Based on List Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "513224b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter IS IN List values\n",
    "li=[\"OH\",\"CA\",\"DE\"]\n",
    "df.filter(df.state.isin(li)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d696e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n",
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter NOT IS IN List values\n",
    "#These show all records with NY (NY is not part of the list)\n",
    "df.filter(~df.state.isin(li)).show()\n",
    "df.filter(df.state.isin(li)==False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498ed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is null or null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6460fbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|James| null|     M|\n",
      "| Anna|   NY|     F|\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (\"James\",None,\"M\"),\n",
    "    (\"Anna\",\"NY\",\"F\"),\n",
    "    (\"Julia\",None,None)\n",
    "  ]\n",
    "\n",
    "columns = [\"name\",\"state\",\"gender\"]\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "85513a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|James| null|     M|\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n",
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|James| null|     M|\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n",
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|James| null|     M|\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"state is NULL\").show()\n",
    "df.filter(df.state.isNull()).show()\n",
    "df.filter(col(\"state\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba6bf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Rows with IS NOT NULL or isNotNull\n",
    "\n",
    "#isNotNull() is used to filter rows that are NOT NULL in DataFrame \n",
    "#columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eb91102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+\n",
      "|name|state|gender|\n",
      "+----+-----+------+\n",
      "|Anna|   NY|     F|\n",
      "+----+-----+------+\n",
      "\n",
      "+----+-----+------+\n",
      "|name|state|gender|\n",
      "+----+-----+------+\n",
      "|Anna|   NY|     F|\n",
      "+----+-----+------+\n",
      "\n",
      "+----+-----+------+\n",
      "|name|state|gender|\n",
      "+----+-----+------+\n",
      "|Anna|   NY|     F|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.filter(\"state IS NOT NULL\").show()\n",
    "df.filter(df.state.isNotNull()).show()\n",
    "df.filter(col(\"state\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2b2f3044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark SQL Filter Rows with NULL Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e70e4ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|James| null|     M|\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n",
      "+-----+-----+------+\n",
      "| name|state|gender|\n",
      "+-----+-----+------+\n",
      "|Julia| null|  null|\n",
      "+-----+-----+------+\n",
      "\n",
      "+----+-----+------+\n",
      "|name|state|gender|\n",
      "+----+-----+------+\n",
      "|Anna|   NY|     F|\n",
      "+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"DATA\")\n",
    "spark.sql(\"SELECT * FROM DATA where STATE IS NULL\").show()\n",
    "spark.sql(\"SELECT * FROM DATA where STATE IS NULL AND GENDER IS NULL\").show()\n",
    "spark.sql(\"SELECT * FROM DATA where STATE IS NOT NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08125c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to SQL GROUP BY clause, \n",
    "# PySpark groupBy() function is used to collect the identical data into groups on \n",
    "# DataFrame and perform aggregate functions on the grouped data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d3d2a",
   "metadata": {},
   "source": [
    "we perform groupBy() on PySpark Dataframe, it returns GroupedData object which contains below aggregate functions.\n",
    "\n",
    "1.count() - Returns the count of rows for each group.\n",
    "\n",
    "2.mean() - Returns the mean of values for each group.\n",
    "\n",
    "3.max() - Returns the maximum of values for each group.\n",
    "\n",
    "4.min() - Returns the minimum of values for each group.\n",
    "\n",
    "5.sum() - Returns the total for values for each group.\n",
    "\n",
    "6.avg() - Returns the average for values for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5929b7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5387f9ea",
   "metadata": {},
   "source": [
    "PySpark groupBy and aggregate on DataFrame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a2a78302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "86e91941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: string, count: bigint]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"department\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fb0aa4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: string, min(salary): bigint]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"department\").min(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "20ea2e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: string, avg(salary): double]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"department\").avg( \"salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79816b",
   "metadata": {},
   "source": [
    "PySpark groupBy and aggregate on multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9c6bcc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+----------+\n",
      "|department|state|sum(salary)|sum(bonus)|\n",
      "+----------+-----+-----------+----------+\n",
      "|Finance   |NY   |162000     |34000     |\n",
      "|Marketing |NY   |91000      |21000     |\n",
      "|Sales     |CA   |81000      |23000     |\n",
      "|Marketing |CA   |80000      |18000     |\n",
      "|Finance   |CA   |189000     |47000     |\n",
      "|Sales     |NY   |176000     |30000     |\n",
      "+----------+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#//GroupBy on multiple columns\n",
    "df.groupBy(\"department\",\"state\") \\\n",
    "    .sum(\"salary\",\"bonus\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83428bbd",
   "metadata": {},
   "source": [
    "# PySpark Join Types | Join Two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf07439",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. PySpark Join Syntax\n",
    "\n",
    "join(self, other, on=None, how=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "Join String\tEquivalent SQL Join\n",
    "inner\tINNER JOIN\n",
    "outer, full, fullouter, full_outer\tFULL OUTER JOIN\n",
    "left, leftouter, left_outer\tLEFT JOIN\n",
    "right, rightouter, right_outer\tRIGHT JOIN\n",
    "cross\t\n",
    "anti, leftanti, left_anti\t\n",
    "semi, leftsemi, left_semi\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ed074e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b7339",
   "metadata": {},
   "source": [
    "3. PySpark Inner Join DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd9bde24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
    "     .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33773057",
   "metadata": {},
   "source": [
    "4. PySpark Full Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c7ad9df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
    "    .show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf0d26a",
   "metadata": {},
   "source": [
    "5. PySpark Left Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bc905d85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a2eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.PySpark right Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4ef5e387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Left Semi Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d4ae3af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Left Anti Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9146d684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|6     |Brown|2              |2010       |50         |      |-1    |\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79632bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. PySpark Self Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "09de3285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------------+\n",
      "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
      "+------+--------+---------------+-----------------+\n",
      "|2     |Rose    |1              |Smith            |\n",
      "|3     |Williams|1              |Smith            |\n",
      "|4     |Jones   |2              |Rose             |\n",
      "|5     |Brown   |2              |Rose             |\n",
      "|6     |Brown   |2              |Rose             |\n",
      "+------+--------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
    "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
    "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
    "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
    "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb632181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SQL Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "25208143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "\n",
    "joinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)\n",
    "\n",
    "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark SQL Join on multiple DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2,df1.id1 == df2.id2,\"inner\") \\\n",
    "   .join(df3,df1.id1 == df3.id3,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5148a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06b61ead",
   "metadata": {},
   "source": [
    "# PySpark UDF Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348d83f",
   "metadata": {},
   "source": [
    "UDF’s a.k.a User Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3e010b",
   "metadata": {},
   "source": [
    "PySpark UDF’s are similar to UDF on traditional databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764edd9",
   "metadata": {},
   "source": [
    "In PySpark, you create a function in a Python syntax and wrap it with PySpark SQL \"udf()\" or register it as udf and use it on DataFrame and SQL respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd748770",
   "metadata": {},
   "source": [
    "Create PySpark UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138bdc3e",
   "metadata": {},
   "source": [
    "2.1 Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "85fd681c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91086f9f",
   "metadata": {},
   "source": [
    "2.2 Create a Python Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e14dacbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c8c64d",
   "metadata": {},
   "source": [
    "2.3 Convert a Python function to PySpark UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "67c2729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Converting function to UDF \"\"\"\n",
    "convertUDF = udf(lambda z: convertCase(z),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5ca17d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Converting function to UDF \n",
    "StringType() is by default hence not required \"\"\"\n",
    "convertUDF = udf(lambda z: convertCase(z)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0072f27",
   "metadata": {},
   "source": [
    "3. Using UDF with DataFrame\n",
    "\n",
    "3 .1 Using UDF with PySpark DataFrame select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b344709f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4622bbae",
   "metadata": {},
   "source": [
    "3.2 Using UDF with PySpark DataFrame withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "920c588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upperCase(str):\n",
    "    return str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cb9d418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|Name        |Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|1    |john jones  |JOHN JONES   |\n",
      "|2    |tracey smith|TRACEY SMITH |\n",
      "|3    |amy sanders |AMY SANDERS  |\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upperCaseUDF = udf(lambda z:upperCase(z),StringType())   \n",
    "\n",
    "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88b6ff",
   "metadata": {},
   "source": [
    "# PySpark When Otherwise | SQL Case When Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b00be",
   "metadata": {},
   "source": [
    "PySpark When Otherwise – when() is a SQL function that returns a Column type and otherwise() is a function of Column, if otherwise() is not used, it returns a None/NULL value.\n",
    "\n",
    "PySpark SQL Case When – This is similar to SQL expression, Usage: CASE WHEN cond1 THEN result WHEN cond2 THEN result... ELSE result END."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2b79cebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M| 60000|\n",
      "|Michael|     M| 70000|\n",
      "| Robert|  null|400000|\n",
      "|  Maria|     F|500000|\n",
      "|    Jen|      |  null|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ebe0c",
   "metadata": {},
   "source": [
    "1. Using when() otherwise() on PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fbc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when(condition).otherwise(default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca2362",
   "metadata": {},
   "source": [
    "when() function take 2 parameters, first param takes a condition and second takes a literal value or Column, if condition evaluates to true then it returns a value from second param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "52764598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  null|400000|          |\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  null|          |\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when \n",
    "df2 = df.withColumn(\"new_gender\", when(df.gender == \"M\",\"Male\")\n",
    "                                 .when(df.gender == \"F\",\"Female\")\n",
    "                                 .when(df.gender.isNull() ,\"\")\n",
    "                                 .otherwise(df.gender))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "15eaa688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using with select()\n",
    "df2=df.select(col(\"*\"),when(df.gender == \"M\",\"Male\")\n",
    "                  .when(df.gender == \"F\",\"Female\")\n",
    "                  .when(df.gender.isNull() ,\"\")\n",
    "                  .otherwise(df.gender).alias(\"new_gender\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a15697",
   "metadata": {},
   "source": [
    "2. PySpark SQL Case When on DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fec74f",
   "metadata": {},
   "source": [
    "Syntax of SQL CASE WHEN ELSE END\n",
    "\n",
    "\n",
    "CASE\n",
    "    WHEN condition1 THEN result_value1\n",
    "    WHEN condition2 THEN result_value2\n",
    "    -----\n",
    "    -----\n",
    "    ELSE result\n",
    "END;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77312451",
   "metadata": {},
   "outputs": [],
   "source": [
    "-\"CASE\" is the start of the expression\n",
    "-Clause \"WHEN\" takes a condition, if condition true it returns a value from THEN\n",
    "-If the condition is false it goes to the next condition and so on.\n",
    "-If none of the condition matches, it returns a value from the \"ELSE\" clause.\n",
    "-END is to end the expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55cb8b",
   "metadata": {},
   "source": [
    "2.1 Using Case When Else on DataFrame using withColumn() & select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ebc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "#Using Case When on withColumn()\n",
    "df3 = df.withColumn(\"new_gender\", expr(\"CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "               \"ELSE gender END\"))\n",
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1f8374ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Case When on select()\n",
    "df4 = df.select(col(\"*\"), expr(\"CASE WHEN gender = 'M' THEN 'Male' \" +\n",
    "           \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "           \"ELSE gender END\").alias(\"new_gender\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c149c6",
   "metadata": {},
   "source": [
    "2.2 Using Case When on SQL Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2d4d855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   name|new_gender|\n",
      "+-------+----------+\n",
      "|  James|      Male|\n",
      "|Michael|      Male|\n",
      "| Robert|          |\n",
      "|  Maria|    Female|\n",
      "|    Jen|          |\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select name, CASE WHEN gender = 'M' THEN 'Male' \" + \n",
    "               \"WHEN gender = 'F' THEN 'Female' WHEN gender IS NULL THEN ''\" +\n",
    "              \"ELSE gender END as new_gender from EMP\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbf6afe",
   "metadata": {},
   "source": [
    "2.3. Multiple Conditions using & and | operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ff5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.withColumn(\"new_column\", when(col(\"code\") == \"a\" | col(\"code\") == \"d\", \"A\")\n",
    "      .when(col(\"code\") == \"b\" & col(\"amt\") == \"4\", \"B\")\n",
    "      .otherwise(\"A1\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "+---+----+---+----------+\n",
    "| id|code|amt|new_column|\n",
    "+---+----+---+----------+\n",
    "| 66|   a|  4|         A|\n",
    "| 67|   a|  0|         A|\n",
    "| 70|   b|  4|         B|\n",
    "| 71|   d|  4|         A|\n",
    "+---+----+---+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15efcd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c986b76b",
   "metadata": {},
   "source": [
    "# PySpark Create DataFrame from List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ae8a5cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Finance', 10), ('Marketing', 20), ('Sales', 30), ('IT', 40)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), \n",
    "        (\"Marketing\",20), \n",
    "        (\"Sales\",30), \n",
    "        (\"IT\",40) \n",
    "      ]\n",
    "dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "48769f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed42aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, let’s add a columns using Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c934c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "deptSchema = StructType([       \n",
    "    StructField('firstname', StringType(), True),\n",
    "    StructField('middlename', StringType(), True),\n",
    "    StructField('lastname', StringType(), True)\n",
    "])\n",
    "\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptSchema)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6d1e01",
   "metadata": {},
   "source": [
    "# PySpark Create DataFrame From Dictionary (Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "709ac898",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDictionary = [\n",
    "        ('James',{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',{'hair':'brown','eye':None}),\n",
    "        ('Robert',{'hair':'red','eye':'black'}),\n",
    "        ('Washington',{'hair':'red','eye':'grey'}),\n",
    "        ('Jefferson',{'hair':'red','eye':''})\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3bcacec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-----------------------------+\n",
      "|name      |properties                   |\n",
      "+----------+-----------------------------+\n",
      "|James     |{eye -> brown, hair -> black}|\n",
      "|Michael   |{eye -> null, hair -> brown} |\n",
      "|Robert    |{eye -> black, hair -> red}  |\n",
      "|Washington|{eye -> grey, hair -> red}   |\n",
      "|Jefferson |{eye -> , hair -> red}       |\n",
      "+----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=dataDictionary, schema = [\"name\",\"properties\"])\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f7f97",
   "metadata": {},
   "source": [
    "Create a DataFrame Dictionary Column Using StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec848f",
   "metadata": {},
   "source": [
    "PySpark doesn’t have a Dictionary type instead it uses MapType to store the dictionary object.\n",
    "\n",
    "MapType(StringType(),StringType()) – Here both key and value is a StringType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e15c6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
    "schema = StructType([\n",
    "  StructField('name', StringType(), True),\n",
    "  StructField('properties', MapType(StringType(),StringType()),True)\n",
    "])\n",
    "df2 = spark.createDataFrame(data=dataDictionary, schema = schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d148d48",
   "metadata": {},
   "source": [
    "Extract Values from DataFrame Dictionary Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "234d747f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+\n",
      "|      hair|  eye|   _3|\n",
      "+----------+-----+-----+\n",
      "|     James|black|brown|\n",
      "|   Michael|brown| null|\n",
      "|    Robert|  red|black|\n",
      "|Washington|  red| grey|\n",
      "| Jefferson|  red|     |\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.rdd.map(lambda x: \n",
    "    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])\n",
    "  ).toDF([\"hair\",\"eye\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f90d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fb826a7",
   "metadata": {},
   "source": [
    "PySpark – Cast Column Type With Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cast Column Type With Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9e5d8986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------------+-----------+------+------+\n",
      "|firstname|age|jobStartDate|isGraduated|gender|salary|\n",
      "+---------+---+------------+-----------+------+------+\n",
      "|James    |34 |2006-01-01  |true       |M     |3000.6|\n",
      "|Michael  |33 |1980-01-10  |true       |F     |3300.8|\n",
      "|Robert   |37 |06-01-1992  |false      |M     |5000.5|\n",
      "+---------+---+------------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "simpleData = [(\"James\",34,\"2006-01-01\",\"true\",\"M\",3000.60),\n",
    "    (\"Michael\",33,\"1980-01-10\",\"true\",\"F\",3300.80),\n",
    "    (\"Robert\",37,\"06-01-1992\",\"false\",\"M\",5000.50)\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"age\",\"jobStartDate\",\"isGraduated\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4400971f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[firstname: string, age: int, jobStartDate: string, isGraduated: string, gender: string, salary: double]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert String to Integer Type\n",
    "df.withColumn(\"age\",df.age.cast(IntegerType()))\n",
    "df.withColumn(\"age\",df.age.cast('int'))\n",
    "df.withColumn(\"age\",df.age.cast('integer'))\n",
    "\n",
    "# Using select\n",
    "#df.select(col(\"age\").cast('int').alias(\"age\"))\n",
    "\n",
    "#Using selectExpr()\n",
    "#df.selectExpr(\"cast(age as int) age\")\n",
    "\n",
    "#Using with spark.sql()\n",
    "#spark.sql(\"SELECT INT(age),BOOLEAN(isGraduated),DATE(jobStartDate) from CastExample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b02ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "87006505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\r\n",
      "---------------------------------- -------------------\r\n",
      "alabaster                          0.7.12\r\n",
      "anaconda-client                    1.7.2\r\n",
      "anaconda-navigator                 2.0.3\r\n",
      "anaconda-project                   0.9.1\r\n",
      "anyio                              2.2.0\r\n",
      "appdirs                            1.4.4\r\n",
      "applaunchservices                  0.2.1\r\n",
      "appnope                            0.1.2\r\n",
      "appscript                          1.1.2\r\n",
      "argh                               0.26.2\r\n",
      "argon2-cffi                        20.1.0\r\n",
      "asn1crypto                         1.4.0\r\n",
      "astroid                            2.5\r\n",
      "astropy                            4.2.1\r\n",
      "async-generator                    1.10\r\n",
      "atomicwrites                       1.4.0\r\n",
      "attrs                              20.3.0\r\n",
      "autopep8                           1.5.6\r\n",
      "Babel                              2.9.0\r\n",
      "backcall                           0.2.0\r\n",
      "backports.functools-lru-cache      1.6.4\r\n",
      "backports.shutil-get-terminal-size 1.0.0\r\n",
      "backports.tempfile                 1.0\r\n",
      "backports.weakref                  1.0.post1\r\n",
      "beautifulsoup4                     4.9.3\r\n",
      "bitarray                           1.9.2\r\n",
      "bkcharts                           0.2\r\n",
      "black                              19.10b0\r\n",
      "bleach                             3.3.0\r\n",
      "bokeh                              2.3.2\r\n",
      "boto                               2.49.0\r\n",
      "Bottleneck                         1.3.2\r\n",
      "brotlipy                           0.7.0\r\n",
      "certifi                            2020.12.5\r\n",
      "cffi                               1.14.5\r\n",
      "chardet                            4.0.0\r\n",
      "click                              7.1.2\r\n",
      "cloudpickle                        1.6.0\r\n",
      "clyent                             1.2.2\r\n",
      "colorama                           0.4.4\r\n",
      "conda                              4.10.1\r\n",
      "conda-build                        3.21.4\r\n",
      "conda-content-trust                0+unknown\r\n",
      "conda-package-handling             1.7.3\r\n",
      "conda-repo-cli                     1.0.4\r\n",
      "conda-token                        0.3.0\r\n",
      "conda-verify                       3.4.2\r\n",
      "contextlib2                        0.6.0.post1\r\n",
      "cryptography                       3.4.7\r\n",
      "cycler                             0.10.0\r\n",
      "Cython                             0.29.23\r\n",
      "cytoolz                            0.11.0\r\n",
      "dask                               2021.4.0\r\n",
      "decorator                          5.0.6\r\n",
      "defusedxml                         0.7.1\r\n",
      "diff-match-patch                   20200713\r\n",
      "distributed                        2021.4.0\r\n",
      "docutils                           0.17\r\n",
      "entrypoints                        0.3\r\n",
      "et-xmlfile                         1.0.1\r\n",
      "fastcache                          1.1.0\r\n",
      "filelock                           3.0.12\r\n",
      "flake8                             3.9.0\r\n",
      "Flask                              1.1.2\r\n",
      "fsspec                             0.9.0\r\n",
      "future                             0.18.2\r\n",
      "gevent                             21.1.2\r\n",
      "glob2                              0.7\r\n",
      "gmpy2                              2.0.8\r\n",
      "greenlet                           1.0.0\r\n",
      "h5py                               2.10.0\r\n",
      "HeapDict                           1.0.1\r\n",
      "html5lib                           1.1\r\n",
      "idna                               2.10\r\n",
      "imageio                            2.9.0\r\n",
      "imagesize                          1.2.0\r\n",
      "importlib-metadata                 3.10.0\r\n",
      "iniconfig                          1.1.1\r\n",
      "intervaltree                       3.1.0\r\n",
      "ipykernel                          5.3.4\r\n",
      "ipython                            7.22.0\r\n",
      "ipython-genutils                   0.2.0\r\n",
      "ipywidgets                         7.6.3\r\n",
      "isort                              5.8.0\r\n",
      "itsdangerous                       1.1.0\r\n",
      "jdcal                              1.4.1\r\n",
      "jedi                               0.17.2\r\n",
      "Jinja2                             2.11.3\r\n",
      "joblib                             1.0.1\r\n",
      "json5                              0.9.5\r\n",
      "jsonschema                         3.2.0\r\n",
      "jupyter                            1.0.0\r\n",
      "jupyter-client                     6.1.12\r\n",
      "jupyter-console                    6.4.0\r\n",
      "jupyter-core                       4.7.1\r\n",
      "jupyter-packaging                  0.7.12\r\n",
      "jupyter-server                     1.4.1\r\n",
      "jupyterlab                         3.0.14\r\n",
      "jupyterlab-pygments                0.1.2\r\n",
      "jupyterlab-server                  2.4.0\r\n",
      "jupyterlab-widgets                 1.0.0\r\n",
      "keyring                            22.3.0\r\n",
      "kiwisolver                         1.3.1\r\n",
      "lazy-object-proxy                  1.6.0\r\n",
      "libarchive-c                       2.9\r\n",
      "llvmlite                           0.36.0\r\n",
      "locket                             0.2.1\r\n",
      "lxml                               4.6.3\r\n",
      "MarkupSafe                         1.1.1\r\n",
      "matplotlib                         3.3.4\r\n",
      "mccabe                             0.6.1\r\n",
      "mistune                            0.8.4\r\n",
      "mkl-fft                            1.3.0\r\n",
      "mkl-random                         1.2.1\r\n",
      "mkl-service                        2.3.0\r\n",
      "mock                               4.0.3\r\n",
      "more-itertools                     8.7.0\r\n",
      "mpmath                             1.2.1\r\n",
      "msgpack                            1.0.2\r\n",
      "multipledispatch                   0.6.0\r\n",
      "mypy-extensions                    0.4.3\r\n",
      "navigator-updater                  0.2.1\r\n",
      "nbclassic                          0.2.6\r\n",
      "nbclient                           0.5.3\r\n",
      "nbconvert                          6.0.7\r\n",
      "nbformat                           5.1.3\r\n",
      "nest-asyncio                       1.5.1\r\n",
      "networkx                           2.5\r\n",
      "nltk                               3.6.1\r\n",
      "nose                               1.3.7\r\n",
      "notebook                           6.3.0\r\n",
      "numba                              0.53.1\r\n",
      "numexpr                            2.7.3\r\n",
      "numpy                              1.20.1\r\n",
      "numpydoc                           1.1.0\r\n",
      "olefile                            0.46\r\n",
      "openpyxl                           3.0.7\r\n",
      "packaging                          20.9\r\n",
      "pandas                             1.2.4\r\n",
      "pandocfilters                      1.4.3\r\n",
      "parso                              0.7.0\r\n",
      "partd                              1.2.0\r\n",
      "path                               15.1.2\r\n",
      "pathlib2                           2.3.5\r\n",
      "pathspec                           0.7.0\r\n",
      "patsy                              0.5.1\r\n",
      "pep8                               1.7.1\r\n",
      "pexpect                            4.8.0\r\n",
      "pickleshare                        0.7.5\r\n",
      "Pillow                             8.2.0\r\n",
      "pip                                21.0.1\r\n",
      "pkginfo                            1.7.0\r\n",
      "pluggy                             0.13.1\r\n",
      "ply                                3.11\r\n",
      "prometheus-client                  0.10.1\r\n",
      "prompt-toolkit                     3.0.17\r\n",
      "psutil                             5.8.0\r\n",
      "psycopg2                           2.9.1\r\n",
      "ptyprocess                         0.7.0\r\n",
      "py                                 1.10.0\r\n",
      "pycodestyle                        2.6.0\r\n",
      "pycosat                            0.6.3\r\n",
      "pycparser                          2.20\r\n",
      "pycurl                             7.43.0.6\r\n",
      "pydocstyle                         6.0.0\r\n",
      "pyerfa                             1.7.3\r\n",
      "pyflakes                           2.2.0\r\n",
      "Pygments                           2.8.1\r\n",
      "pylint                             2.7.4\r\n",
      "pyls-black                         0.4.6\r\n",
      "pyls-spyder                        0.3.2\r\n",
      "pyodbc                             4.0.0-unsupported\r\n",
      "pyOpenSSL                          20.0.1\r\n",
      "pyparsing                          2.4.7\r\n",
      "pyrsistent                         0.17.3\r\n",
      "PySocks                            1.7.1\r\n",
      "pyspark                            3.1.2\r\n",
      "pytest                             6.2.3\r\n",
      "python-dateutil                    2.8.1\r\n",
      "python-jsonrpc-server              0.4.0\r\n",
      "python-language-server             0.36.2\r\n",
      "pytz                               2021.1\r\n",
      "PyWavelets                         1.1.1\r\n",
      "PyYAML                             5.4.1\r\n",
      "pyzmq                              20.0.0\r\n",
      "QDarkStyle                         2.8.1\r\n",
      "QtAwesome                          1.0.2\r\n",
      "qtconsole                          5.0.3\r\n",
      "QtPy                               1.9.0\r\n",
      "regex                              2021.4.4\r\n",
      "requests                           2.25.1\r\n",
      "rope                               0.18.0\r\n",
      "Rtree                              0.9.7\r\n",
      "ruamel-yaml-conda                  0.15.100\r\n",
      "scikit-image                       0.18.1\r\n",
      "scikit-learn                       0.24.1\r\n",
      "scipy                              1.6.2\r\n",
      "seaborn                            0.11.1\r\n",
      "Send2Trash                         1.5.0\r\n",
      "setuptools                         52.0.0.post20210125\r\n",
      "simplegeneric                      0.8.1\r\n",
      "singledispatch                     0.0.0\r\n",
      "six                                1.15.0\r\n",
      "sniffio                            1.2.0\r\n",
      "snowballstemmer                    2.1.0\r\n",
      "sortedcollections                  2.1.0\r\n",
      "sortedcontainers                   2.3.0\r\n",
      "soupsieve                          2.2.1\r\n",
      "Sphinx                             4.0.1\r\n",
      "sphinxcontrib-applehelp            1.0.2\r\n",
      "sphinxcontrib-devhelp              1.0.2\r\n",
      "sphinxcontrib-htmlhelp             1.0.3\r\n",
      "sphinxcontrib-jsmath               1.0.1\r\n",
      "sphinxcontrib-qthelp               1.0.3\r\n",
      "sphinxcontrib-serializinghtml      1.1.4\r\n",
      "sphinxcontrib-websupport           1.2.4\r\n",
      "spyder                             4.2.5\r\n",
      "spyder-kernels                     1.10.2\r\n",
      "SQLAlchemy                         1.4.7\r\n",
      "statsmodels                        0.12.2\r\n",
      "sympy                              1.8\r\n",
      "tables                             3.6.1\r\n",
      "tblib                              1.7.0\r\n",
      "terminado                          0.9.4\r\n",
      "testpath                           0.4.4\r\n",
      "textdistance                       4.2.1\r\n",
      "threadpoolctl                      2.1.0\r\n",
      "three-merge                        0.1.1\r\n",
      "tifffile                           2020.10.1\r\n",
      "toml                               0.10.2\r\n",
      "toolz                              0.11.1\r\n",
      "tornado                            6.1\r\n",
      "tqdm                               4.59.0\r\n",
      "traitlets                          5.0.5\r\n",
      "typed-ast                          1.4.2\r\n",
      "typing-extensions                  3.7.4.3\r\n",
      "ujson                              4.0.2\r\n",
      "unicodecsv                         0.14.1\r\n",
      "urllib3                            1.26.4\r\n",
      "watchdog                           1.0.2\r\n",
      "wcwidth                            0.2.5\r\n",
      "webencodings                       0.5.1\r\n",
      "Werkzeug                           1.0.1\r\n",
      "wheel                              0.36.2\r\n",
      "widgetsnbextension                 3.5.1\r\n",
      "wrapt                              1.12.1\r\n",
      "wurlitzer                          2.1.0\r\n",
      "xlrd                               2.0.1\r\n",
      "XlsxWriter                         1.3.8\r\n",
      "xlwings                            0.23.0\r\n",
      "xlwt                               1.3.0\r\n",
      "xmltodict                          0.12.0\r\n",
      "yapf                               0.31.0\r\n",
      "zict                               2.0.0\r\n",
      "zipp                               3.4.1\r\n",
      "zope.event                         4.5.0\r\n",
      "zope.interface                     5.3.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "26bd2db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze >requirement.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e3fc27",
   "metadata": {},
   "source": [
    "# PySpark SQL Date and Timestamp Functions\n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-sql-date-and-timestamp-functions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc221d",
   "metadata": {},
   "source": [
    "Date and Time are very important if you are using PySpark for ETL. Most of all these functions accept input as, Date type, Timestamp type, or String. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858ee64",
   "metadata": {},
   "source": [
    "DateType default format is yyyy-MM-dd \n",
    "TimestampType default format is yyyy-MM-dd HH:mm:ss.SSSS\n",
    "Returns null if the input is a string that can not be cast to Date or Timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dfd22a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark SQL Date and Timestamp Functions Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ef420649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|     input|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('SparkByExamples.com') \\\n",
    "            .getOrCreate()\n",
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81627b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e94dffaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2021-09-07|\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#current_date()\n",
    "df.select(current_date().alias(\"current_date\")).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ab03f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6d064a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|     input|date_format|\n",
      "+----------+-----------+\n",
      "|2020-02-01| 02-01-2020|\n",
      "|2019-03-01| 03-01-2019|\n",
      "|2021-03-01| 03-01-2021|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#date_format()\n",
    "df.select(col(\"input\"), \n",
    "    date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4713d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b72d1c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|     input|   to_date|\n",
      "+----------+----------+\n",
      "|2020-02-01|2020-02-01|\n",
      "|2019-03-01|2019-03-01|\n",
      "|2021-03-01|2021-03-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to_date()\n",
    "# Below example converts string in date format yyyy-MM-dd to \n",
    "#a DateType yyyy-MM-dd using to_date()\n",
    "df.select(col(\"input\"), \n",
    "    to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\") \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datediff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5f54f305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|     input|datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|     584|\n",
      "|2019-03-01|     921|\n",
      "|2021-03-01|     190|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#datediff()\n",
    "# The below example returns the difference between \n",
    "#two dates using datediff()\n",
    "df.select(col(\"input\"), \n",
    "    datediff(current_date(),col(\"input\")).alias(\"datediff\")  \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# months_between()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2962d1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|     input|months_between|\n",
      "+----------+--------------+\n",
      "|2020-02-01|   19.19354839|\n",
      "|2019-03-01|   30.19354839|\n",
      "|2021-03-01|    6.19354839|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#months_between()\n",
    "# The below example returns the months between two dates \n",
    "# using months_between().\n",
    "df.select(col(\"input\"), \n",
    "    months_between(current_date(),col(\"input\")).alias(\"months_between\")  \n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3f78a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trunc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trunc()\n",
    "# The below example truncates the date at a specified unit \n",
    "#using trunc().\n",
    "df.select(col(\"input\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"), \n",
    "    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_months() , date_add(), date_sub()\n",
    "#  we are adding and subtracting date and month from a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ce484c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|     input|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n",
      "|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n",
      "|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#add_months() , date_add(), date_sub()\n",
    "df.select(col(\"input\"), \n",
    "    add_months(col(\"input\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"input\"),4).alias(\"date_add\"), \n",
    "    date_sub(col(\"input\"),4).alias(\"date_sub\") \n",
    "  ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de94b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# year(), month(), month(),next_day(), weekofyear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d7954569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+----------+----------+\n",
      "|     input|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-01|2020|    2|2020-02-02|         5|\n",
      "|2019-03-01|2019|    3|2019-03-03|         9|\n",
      "|2021-03-01|2021|    3|2021-03-07|         9|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"), \n",
    "     year(col(\"input\")).alias(\"year\"), \n",
    "     month(col(\"input\")).alias(\"month\"), \n",
    "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"input\")).alias(\"weekofyear\") \n",
    "  ).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43697f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dayofweek(), dayofmonth(), dayofyear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dda3fc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+\n",
      "|     input|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+---------+----------+---------+\n",
      "|2020-02-01|        7|         1|       32|\n",
      "|2019-03-01|        6|         1|       60|\n",
      "|2021-03-01|        2|         1|       60|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"input\"),  \n",
    "     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"input\")).alias(\"dayofyear\"), \n",
    "  ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0846c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_timestamp()\n",
    "# Following are the Timestamp Functions that you can use on SQL \n",
    "# and on DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0df24b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below example returns the current timestamp in spark default \n",
    "# format yyyy-MM-dd HH:mm:ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4de35a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|current_timestamp         |\n",
      "+--------------------------+\n",
      "|2021-09-07 04:38:00.292697|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#current_timestamp()\n",
    "df2.select(current_timestamp().alias(\"current_timestamp\")\n",
    "  ).show(1,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75917a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts string timestamp to Timestamp type format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ba5e4841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|input                  |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to_timestamp()\n",
    "df2.select(col(\"input\"), \n",
    "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\") \n",
    "  ).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2271953",
   "metadata": {},
   "source": [
    "# PySpark Concatenate Columns\n",
    "https://sparkbyexamples.com/pyspark/pyspark-concatenate-columns/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09803e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Concatenate Using concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat() function of Pyspark SQL is used to concatenate multiple\n",
    "# DataFrame columns into a single column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010bec8b",
   "metadata": {},
   "source": [
    "pyspark.sql.functions.concat(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1a9a35d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+------+\n",
      "|FullName      |dob       |gender|salary|\n",
      "+--------------+----------+------+------+\n",
      "|JamesSmith    |1991-04-01|M     |3000  |\n",
      "|MichaelRose   |2000-05-19|M     |4000  |\n",
      "|RobertWilliams|1978-09-05|M     |4000  |\n",
      "|MariaAnneJones|1967-12-01|F     |4000  |\n",
      "|JenMaryBrown  |1980-02-17|F     |-1    |\n",
      "+--------------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat,col\n",
    "data = [('James','','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','','2000-05-19','M',4000),\n",
    "  ('Robert','','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df2=df.select(concat(df.firstname,df.middlename,df.lastname)\n",
    "              .alias(\"FullName\"),\"dob\",\"gender\",\"salary\") # concatenated three input string \n",
    "# columns(firstname, middlename, lastname) into a single string column(FullName)\n",
    "df2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db319c",
   "metadata": {},
   "source": [
    "# PySpark split() Column into Multiple Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f49fca5",
   "metadata": {},
   "source": [
    "Syntax:\n",
    "\n",
    "pyspark.sql.functions.split(str, pattern, limit=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b173f4",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "\n",
    "str – a string expression to split\n",
    "\n",
    "pattern – a string representing a regular expression.\n",
    "\n",
    "limit –an integer that controls the number of times pattern is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "993852c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"sparkbyexamples\").getOrCreate()\n",
    "data=data = [('James','','Smith','1991-04-01'),\n",
    "  ('Michael','Rose','','2000-05-19'),\n",
    "  ('Robert','','Williams','1978-09-05'),\n",
    "  ('Maria','Anne','Jones','1967-12-01'),\n",
    "  ('Jen','Mary','Brown','1980-02-17')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Column using withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "af3c413b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+----------+------+------+----+-----+---+\n",
      "|firstname|middlename|lastname|dob       |gender|salary|year|month|day|\n",
      "+---------+----------+--------+----------+------+------+----+-----+---+\n",
      "|James    |          |Smith   |1991-04-01|M     |3000  |1991|04   |01 |\n",
      "|Michael  |Rose      |        |2000-05-19|M     |4000  |2000|05   |19 |\n",
      "|Robert   |          |Williams|1978-09-05|M     |4000  |1978|09   |05 |\n",
      "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |1967|12   |01 |\n",
      "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |1980|02   |17 |\n",
      "+---------+----------+--------+----------+------+------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('year', split(df['dob'], '-').getItem(0)) \\\n",
    "       .withColumn('month', split(df['dob'], '-').getItem(1)) \\\n",
    "       .withColumn('day', split(df['dob'], '-').getItem(2))\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96811ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8ac377b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From local[5]8\n",
      "parallelize : 6\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize((0,20))\n",
    "print(\"From local[5]\"+str(rdd.getNumPartitions()))\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize((0,25), 6)\n",
    "print(\"parallelize : \"+str(rdd1.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc6505",
   "metadata": {},
   "source": [
    "spark.sparkContext.parallelize(Range(0,20),6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f964e5",
   "metadata": {},
   "source": [
    "Partition 1 : 0 1 2\n",
    "Partition 2 : 3 4 5\n",
    "Partition 3 : 6 7 8 9\n",
    "Partition 4 : 10 11 12\n",
    "Partition 5 : 13 14 15\n",
    "Partition 6 : 16 17 18 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea5e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 RDD repartition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9edb112",
   "metadata": {},
   "source": [
    "Spark RDD repartition() method is used to increase or decrease the partitions. The below example decreases the partitions from 10 to 4 by moving data from all partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "143e7e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 4\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd1.repartition(4)\n",
    "print(\"Repartition size : \"+str(rdd2.getNumPartitions()))\n",
    "rdd2.saveAsTextFile(\"/tmp/re-partition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de216b9a",
   "metadata": {},
   "source": [
    "Partition 1 : 1 6 10 15 19\n",
    "Partition 2 : 2 3 7 11 16\n",
    "Partition 3 : 4 8 12 13 17\n",
    "Partition 4 : 0 5 9 14 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042708c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 RDD coalesce()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d548915",
   "metadata": {},
   "source": [
    "Spark RDD coalesce() is used only to reduce the number of partitions. This is optimized or improved version of repartition() where the movement of the data across the partitions is lower using coalesce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "18e5c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartition size : 4\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd1.coalesce(4)\n",
    "print(\"Repartition size : \"+str(rdd3.getNumPartitions()))\n",
    "rdd3.saveAsTextFile(\"/tmp/coalesce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daaa977",
   "metadata": {},
   "source": [
    "Partition 1 : 0 1 2\n",
    "Partition 2 : 3 4 5 6 7 8 9\n",
    "Partition 4 : 10 11 12 \n",
    "Partition 5 : 13 14 15 16 17 18 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e8345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04f85882",
   "metadata": {},
   "source": [
    "# PySpark RDD Benefits\n",
    "PySpark is widely adapted in Machine learning and Data science community due to it’s advantages compared with traditional python programming.\n",
    "\n",
    "# In-Memory Processing\n",
    "PySpark loads the data from disk and process in memory and keeps the data in memory, this is the main difference between PySpark and Mapreduce (I/O intensive). In between the transformations, we can also cache/persists the RDD in memory to reuse the previous computations.\n",
    "\n",
    "# Immutability\n",
    "PySpark RDD’s are immutable in nature meaning, once RDDs are created you cannot modify. When we apply transformations on RDD, PySpark creates a new RDD and maintains the RDD Lineage.\n",
    "\n",
    "# Fault Tolerance\n",
    "PySpark operates on fault-tolerant data stores on HDFS, S3 e.t.c hence any RDD operation fails, it automatically reloads the data from other partitions. Also, When PySpark applications running on a cluster, PySpark task failures are automatically recovered for a certain number of times (as per the configuration) and finish the application seamlessly.\n",
    "\n",
    "# Lazy Evolution\n",
    "PySpark does not evaluate the RDD transformations as they appear/encountered by Driver instead it keeps the all transformations as it encounters(DAG) and evaluates the all transformation when it sees the first RDD action.\n",
    "\n",
    "# Partitioning\n",
    "When you create RDD from a data, It by default partitions the elements in a RDD. By default it partitions to the number of cores available.\n",
    "\n",
    "#PySpark RDD Limitations\n",
    "PySpark RDDs are not much suitable for applications that make updates to the state store such as storage systems for a web application. For these applications, it is more efficient to use systems that perform traditional update logging and data checkpointing, such as databases. The goal of RDD is to provide an efficient programming model for batch analytics and leave these asynchronous applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11472538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc198526",
   "metadata": {},
   "source": [
    "# RDD Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebac0cfa",
   "metadata": {},
   "source": [
    "flatMap – flatMap() transformation flattens the RDD after applying the function and returns a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f404aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1ada9f",
   "metadata": {},
   "source": [
    "map – map() transformation is used the apply any complex operations like adding a column, updating a column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd7977",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = rdd2.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89786fc",
   "metadata": {},
   "source": [
    "reduceByKey – reduceByKey() merges the values for each key with the function specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539bbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd5 = rdd3.reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570a40d",
   "metadata": {},
   "source": [
    "sortByKey – sortByKey() transformation is used to sort RDD elements on key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd6 = rdd5.map(lambda x: (x[1],x[0])).sortByKey()\n",
    "#Print rdd6 result to console\n",
    "print(rdd6.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3c7d4",
   "metadata": {},
   "source": [
    "filter – filter() transformation is used to filter the records in an RDD. In our example we are filtering all words starts with “a”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd4 = rdd3.filter(lambda x : 'an' in x[1])\n",
    "print(rdd4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef3c0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4b1f5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6fef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Create PySpark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d1a6cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Convert PySpark RDD to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4a829",
   "metadata": {},
   "source": [
    "Converting PySpark RDD to DataFrame can be done using \n",
    "\n",
    "# 1.toDF(), \n",
    "\n",
    "# 2.createDataFrame(). \n",
    "\n",
    "https://sparkbyexamples.com/pyspark/convert-pyspark-rdd-to-dataframe/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662a2db",
   "metadata": {},
   "source": [
    "2.1 Using rdd.toDF() function\n",
    "PySpark provides toDF() function in RDD which can be used to convert RDD into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7c807983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+---------+---+\n",
      "|_1       |_2 |\n",
      "+---------+---+\n",
      "|Finance  |10 |\n",
      "|Marketing|20 |\n",
      "|Sales    |30 |\n",
      "|IT       |40 |\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe36f204",
   "metadata": {},
   "source": [
    "toDF() has another signature that takes arguments to define column names as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "1dde6b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d88c5",
   "metadata": {},
   "source": [
    "# 2.2 Using PySpark createDataFrame() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377051fc",
   "metadata": {},
   "source": [
    "SparkSession class provides createDataFrame() method to create DataFrame and it takes rdd object as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "999e12b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deptDF = spark.createDataFrame(rdd, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "579e4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Using createDataFrame() with StructType schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2f41f584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "deptSchema = StructType([       \n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('dept_id', StringType(), True)\n",
    "])\n",
    "\n",
    "deptDF1 = spark.createDataFrame(rdd, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298f3ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e933a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33805d",
   "metadata": {},
   "source": [
    "PySpark Window functions operate on a group of rows (like frame, partition) and return a single value for every input row. \n",
    "\n",
    "PySpark SQL supports three kinds of window functions:-\n",
    "\n",
    "ranking functions\n",
    "\n",
    "analytic functions\n",
    "\n",
    "aggregate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91d7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15b692e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216618f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PySpark Window Ranking functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe27909",
   "metadata": {},
   "source": [
    "# row_number() \n",
    "window function is used to give the sequential row number starting from 1 to the result of each window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0b5bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b203b58",
   "metadata": {},
   "source": [
    "# rank Window Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdece0d",
   "metadata": {},
   "source": [
    "rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fbd4e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"rank\"\"\"\n",
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf1edb7",
   "metadata": {},
   "source": [
    "# dense_rank Window Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4957c",
   "metadata": {},
   "source": [
    "dense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. \n",
    "\n",
    "This is similar to rank() function difference being rank function leaves gaps in rank when there are ties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae1788",
   "metadata": {},
   "source": [
    "2.4 percent_rank Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb9633f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" percent_rank \"\"\"\n",
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c3b27",
   "metadata": {},
   "source": [
    "2.5 ntile Window Function\n",
    "\n",
    "ntile() window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e92ce181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ntile\"\"\"\n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b802519",
   "metadata": {},
   "source": [
    "4. PySpark Window Aggregate Functions\n",
    "\n",
    "In this section, I will explain how to calculate sum, min, max for each department using PySpark SQL Aggregate window functions and WindowSpec. When working with Aggregate functions, we don’t need to use order by clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfffc2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee26d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask row number \n",
    "pehle where, phir select kaise sequence pta chalega\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219903b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c896589a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef48bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adc84860",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_order' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c34f2e75b5ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morder_payment_orders_join\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'order_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'order_purchase_timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_order_payments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'order_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'order_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_order' is not defined"
     ]
    }
   ],
   "source": [
    "order_payment_orders_join=df_order.select('order_id','order_purchase_timestamp')\\\n",
    ".join(df_order_payments.select('order_id'),on='order_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bece36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ac040",
   "metadata": {},
   "outputs": [],
   "source": [
    "seller_orderreview_join=df_items.select('order_id','product_id','seller_id')\\\n",
    ".join(df_order_reviews.select('review_score','order_id'),on='order_id',how='inner')\n",
    ".join(df_sellers.select('seller_id'),on='seller_id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3152adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b97a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "207da8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2f57ae6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-aa5c07896ec8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-aa5c07896ec8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    val mySpark = SparkSession\\\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "val mySpark = SparkSession\\\n",
    "  .builder()\\\n",
    "  .appName(\"Spark SQL basic example\")\\\n",
    "  .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "  .getOrCreate()\\\n",
    "\n",
    "// For implicit conversions like converting RDDs to DataFrames\n",
    "import mySpark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f25ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280c6e88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ad52d7df305a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ABC    \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"     DEF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"        GHI    \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"col2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spark'"
     ]
    }
   ],
   "source": [
    "import mySpark.implicits._\n",
    "data = Seq((1,\"ABC    \"), (2,\"     DEF\"),(3,\"        GHI    \") )\n",
    "df = data.toDF(\"col1\",\"col2\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1604f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4da032d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-c9db8c39472e>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-c9db8c39472e>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    import org.apache.spark.sql.functions.{trim,ltrim,rtrim,col}\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.sql.functions.{trim,ltrim,rtrim,col}\n",
    "\n",
    "//using withColumn to remove white spaces\n",
    "df.withColumn(\"col2\",trim(col(\"col2\"))).show()\n",
    "df.withColumn(\"col2\",ltrim(col(\"col2\"))).show()\n",
    "df.withColumn(\"col2\",rtrim(col(\"col2\"))).show()\n",
    "\n",
    "//Using select to remove white spaces\n",
    "df.select(col(\"col1\"),trim(col(\"col2\")).as(\"col2\")).show()\n",
    "\n",
    "//Using SQL Expression to remove white spaces\n",
    "df.createOrReplaceTempView(\"TAB\")\n",
    "spark.sql(\"select col1,trim(col2) as col2 from TAB\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2ecce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf70434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276f9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
